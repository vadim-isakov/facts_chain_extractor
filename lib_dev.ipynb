{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for extraction predicates and arguments (noun phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T15:08:07.750971Z",
     "start_time": "2018-10-22T15:08:06.724263Z"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp_srl_framebank.pipeline_default import PipelineDefault  \n",
    "\n",
    "class TextGroups(object):\n",
    "    HOST = 'HOST'\n",
    "    MORPH_PORT = 3333\n",
    "    SYNTAX_PORT = 3334\n",
    "    SRL_PORT = 3335\n",
    "    \n",
    "    \n",
    "    GROUPS_PARAMETERS = {\n",
    "        'predicate': {\n",
    "            'TYPES_HEAD': ['VERB'],\n",
    "            'TYPES_ALL': ['ADJ', 'PART', 'CCONJ', 'ADV', 'ADP'],\n",
    "            'ENTITY_LEMMAS': [\n",
    "                u'не', u'ни'\n",
    "            ]\n",
    "        },\n",
    "        'argument' : {\n",
    "            'TYPES_HEAD': ['NOUN','ADJ', 'CCONJ', 'PRON', 'ADP'],\n",
    "            'TYPES_ALL': ['NOUN','ADJ', 'ADV', 'CCONJ', 'PRON', 'ADP'],\n",
    "            'ENTITY_LEMMAS': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def __init__(self, analyzed_text):\n",
    "        self.pipeline = PipelineDefault(address_morph=(self.HOST, self.MORPH_PORT),\n",
    "                          address_syntax=(self.HOST, self.SYNTAX_PORT),\n",
    "                          address_srl=(self.HOST, self.SRL_PORT))\n",
    "        self.analyzed_text = analyzed_text\n",
    "        self.__analyze_text()\n",
    "        self.children_annotation = self.__extract_children_annotation()\n",
    "        \n",
    "        self.groups = self.__extract_goups()\n",
    "        \n",
    "    def __analyze_text(self):\n",
    "        res = self.pipeline(self.analyzed_text)\n",
    "        self.syntax = res['syntax_dep_tree']\n",
    "        self.morph = res['morph']\n",
    "        self.lemma = res['lemma']\n",
    "        self.tokens = res['tokens']\n",
    "        self.sentences = res['sentences']\n",
    "\n",
    "    \n",
    "    def __extract_children_annotation(self):\n",
    "        children_annotation = []\n",
    "        for sent_pos, sent in enumerate(self.syntax):\n",
    "            sent_children_annotation = [[] for i in range(len(sent))]\n",
    "            for word_pos, word in enumerate(sent):\n",
    "                if word.parent != -1:\n",
    "                    sent_children_annotation[word.parent].append(word_pos)\n",
    "            children_annotation.append(sent_children_annotation)\n",
    "        return children_annotation\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __extract_synt_top(self, synt_sent):\n",
    "        for word_pos, synt_word in enumerate(synt_sent):\n",
    "            if synt_word.link_name == 'ROOT':\n",
    "                return word_pos\n",
    "\n",
    "        \n",
    "        \n",
    "    def __extract_phrases(self, extract_type, sent_pos, current_word_pos, is_top = True, is_father_in_np = False):\n",
    "        types_head = self.GROUPS_PARAMETERS[extract_type]['TYPES_HEAD']\n",
    "        types_all = self.GROUPS_PARAMETERS[extract_type]['TYPES_ALL']\n",
    "        entity_lemmas = self.GROUPS_PARAMETERS[extract_type]['ENTITY_LEMMAS']\n",
    "        \n",
    "        sent_morph = self.morph[sent_pos]\n",
    "        phrase = []\n",
    "        phrases = []\n",
    "        current_word_children = self.children_annotation[sent_pos][current_word_pos]\n",
    "\n",
    "        checking_phrase_types = None\n",
    "        checking_lemmas = []\n",
    "        if is_father_in_np:\n",
    "            checking_phrase_types = types_all\n",
    "            checking_lemmas = entity_lemmas\n",
    "        else:\n",
    "            checking_phrase_types = types_head\n",
    "\n",
    "        if (self.morph[sent_pos][current_word_pos].get('fPOS') in checking_phrase_types) or (self.lemma[sent_pos][current_word_pos] in checking_lemmas):\n",
    "            phrase.append(current_word_pos)\n",
    "            for word_pos in current_word_children:\n",
    "                new_phrase, new_phrases = self.__extract_phrases(extract_type, sent_pos, word_pos, False, True)\n",
    "                phrase += new_phrase\n",
    "                phrases += new_phrases\n",
    "        else:\n",
    "            for word_pos in current_word_children:\n",
    "                new_phrase, new_phrases = self.__extract_phrases(extract_type, sent_pos, word_pos, False)\n",
    "                phrases += new_phrases ## NEW!!\n",
    "                if len(new_phrase) > 0:\n",
    "                    phrases.append(new_phrase)\n",
    "        if is_top:\n",
    "            if len(phrase) > 0:\n",
    "                phrases.append(phrase)\n",
    "            return phrases\n",
    "        else:\n",
    "            return phrase, phrases\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __extract_goups(self):\n",
    "        groups = []\n",
    "        \n",
    "        for sent_pos, synt_sent in enumerate(self.syntax):\n",
    "            sent_top = self.__extract_synt_top(synt_sent)\n",
    "            predicates = self.__extract_phrases('predicate', sent_pos, sent_top)\n",
    "            for predicate in predicates:\n",
    "                \n",
    "                predicate_children = [child_idx for word_idx in predicate\n",
    "                                      for child_idx in self.children_annotation[sent_pos][word_idx]\n",
    "                                      if not child_idx in predicate]\n",
    "                predicate_children = list(set(sorted(predicate_children)))\n",
    "                arguments = []\n",
    "                for predicate_child in predicate_children:\n",
    "                    arguments += self.__extract_phrases('argument', sent_pos, predicate_child)\n",
    "            \n",
    "                groups.append({\n",
    "                    'sent_pos' : sent_pos,\n",
    "                    'arguments': [sorted(arg) for arg in arguments],\n",
    "                    'predicate' : sorted(predicate)\n",
    "                })\n",
    "        return groups\n",
    "            \n",
    "    \n",
    "\n",
    "    def get_pretty_phrase(self, sent_pos, phrase):\n",
    "        sentence_offsets = self.sentences[sent_pos]\n",
    "        sentence_tokens = self.tokens[sentence_offsets.begin:sentence_offsets.end]\n",
    "        return ' '.join([sentence_tokens[word_pos].text for word_pos in phrase])\n",
    "   \n",
    "\n",
    "\n",
    "    def get_pretty(self):\n",
    "        pretty_groups = []\n",
    "        for group in self.groups:  \n",
    "            pretty_groups.append({\n",
    "                'predicate' : self.get_pretty_phrase(group['sent_pos'], group['predicate']),\n",
    "                'arguments' : [self.get_pretty_phrase(group['sent_pos'], arg) for arg in group['arguments']]\n",
    "            })\n",
    "        return pretty_groups\n",
    "\n",
    "\n",
    "groups = TextGroups(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T14:12:10.592195Z",
     "start_time": "2018-10-22T14:12:08.623443Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________\n",
      "подписал\n",
      "\n",
      "Мексиканский боксер Сауль Альварес\n",
      "контракт со стриминговым сервисом\n",
      "\n",
      "\n",
      "___________________\n",
      "имеющейся\n",
      "\n",
      "\n",
      "\n",
      "___________________\n",
      "выступающий\n",
      "\n",
      "в среднем весе\n",
      "поясами чемпиона мира по версиям\n",
      "\n",
      "\n",
      "___________________\n",
      "рассчитанному\n",
      "\n",
      "до года\n",
      "\n",
      "\n",
      "___________________\n",
      "получит\n",
      "\n",
      "По информации\n",
      "минимум\n",
      "до года\n",
      "за боев по соглашению\n",
      "в среднем весе\n",
      "поясами чемпиона мира по версиям\n",
      "летний спортсмен\n",
      "\n",
      "\n",
      "___________________\n",
      "подписал\n",
      "\n",
      "с\n",
      "Майами\n",
      "контракт\n",
      "который\n",
      "Марлинс на лет\n",
      "в году\n",
      "\n",
      "\n",
      "___________________\n",
      "стал рекордным\n",
      "\n",
      "Контракт мексиканца\n",
      "достижение бейсболиста\n",
      "Нью\n",
      "в истории спорта\n",
      "который\n",
      "в году\n",
      "с\n",
      "Майами\n",
      "контракт\n",
      "Марлинс на лет\n",
      "Йорк Янкис Джанкарло Стэнтона\n",
      "\n",
      "\n",
      "___________________\n",
      "проведет\n",
      "\n",
      "с\n",
      "Альварес\n",
      "Первый бой в рамках соглашения\n",
      "декабря\n",
      "\n",
      "\n",
      "___________________\n",
      "встретится\n",
      "\n",
      "во втором среднем весе\n",
      "В поединке за титул\n",
      "с британцем Рокки Филдингом\n",
      "он\n",
      "\n",
      "\n",
      "___________________\n",
      "пройдет в Нью\n",
      "\n",
      "Бой\n",
      "на\n",
      "Йорке\n",
      "Мэдисон Сквер Гарден\n",
      "\n",
      "\n",
      "___________________\n",
      "Канело станет\n",
      "\n",
      "Для\n",
      "поединок\n",
      "м в карьере\n",
      "\n",
      "\n",
      "___________________\n",
      "Ранее выиграл\n",
      "\n",
      "он\n",
      "боев\n",
      "схватки\n",
      "одно поражение от непобежденного американца Флойда Мейвезера\n",
      "решением большинства судей в сентябре го\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('example_text.txt', 'r') as rfile:\n",
    "    text = rfile.read()\n",
    "groups = TextGroups(text)\n",
    "pretty_groups = groups.get_pretty()\n",
    "\n",
    "for group in pretty_groups:\n",
    "    print('___________________')\n",
    "    print(group['predicate'])\n",
    "    print()\n",
    "    for arg in group['arguments']:\n",
    "        print(arg)\n",
    "\n",
    "    print()\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
